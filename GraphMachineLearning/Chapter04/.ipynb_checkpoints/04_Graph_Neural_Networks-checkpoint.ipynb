{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8HAOWyDgPiH"
   },
   "source": [
    "# Shallow methods for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6w1KA97gXao"
   },
   "source": [
    "In this notebook we will exploring a very naive (yet powerful) approach for solving graph-based supervised machine learning. The idea rely on the classic machine learning approach of handcrafted feature extraction.\n",
    "\n",
    "In Chapter 1 you learned how local and global graph properties can be extracted from graphs. Those properties represent the graph itself and bring important informations which can be useful for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5k3sYIRJpMgb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.10.0 Requires-Python >=3.6.0, <3.8.0; 0.11.0 Requires-Python >=3.6.0, <3.8.0; 0.11.1 Requires-Python >=3.6.0, <3.8.0; 0.4.0 Requires-Python >=3.6,<3.7; 0.4.0b0 Requires-Python >=3.6,<3.7; 0.4.1 Requires-Python >=3.5.0, <3.7.0; 0.5.0 Requires-Python >=3.5.0, <3.7.0; 0.6.0 Requires-Python >=3.5.0, <3.7.0; 0.6.1 Requires-Python >=3.5.0, <3.7.0; 0.7.0 Requires-Python >=3.5.0, <3.8.0; 0.7.1 Requires-Python >=3.5.0, <3.8.0; 0.7.2 Requires-Python >=3.5.0, <3.8.0; 0.7.3 Requires-Python >=3.5.0, <3.8.0; 0.8.0 Requires-Python >=3.5.0, <3.8.0; 0.8.1 Requires-Python >=3.5.0, <3.8.0; 0.8.2 Requires-Python >=3.5.0, <3.8.0; 0.8.3 Requires-Python >=3.5.0, <3.8.0; 0.8.4 Requires-Python >=3.5.0, <3.8.0; 0.9.0 Requires-Python >=3.6.0, <3.8.0; 1.0.0 Requires-Python >=3.6.0, <3.8.0; 1.0.0rc1 Requires-Python >=3.6.0, <3.8.0; 1.1.0 Requires-Python >=3.6.0, <3.9.0; 1.2.0 Requires-Python >=3.6.0, <3.9.0; 1.2.1 Requires-Python >=3.6.0, <3.9.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement stellargraph (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for stellargraph\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install stellargraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWL_AuChPcYS"
   },
   "source": [
    "In this demo, we will be using the PROTEINS dataset, already integrated in StellarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "gS5B47T2gWll",
    "outputId": "4020adc2-75b7-4aa5-b480-24d2693a8a74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 05:07:23.891189: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 05:07:24.006601: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-08 05:07:24.006619: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-08 05:07:24.651324: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 05:07:24.651388: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 05:07:24.651394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-08 05:07:25.272913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-08 05:07:25.272936: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-08 05:07:25.272950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fluid): /proc/driver/nvidia/version does not exist\n",
      "2023-03-08 05:07:25.273163: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Each graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dataset = datasets.PROTEINS()\n",
    "display(HTML(dataset.description))\n",
    "graphs, graph_labels = dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDlUMUUFLrjh"
   },
   "source": [
    "To compute the graph metrics, one way is to retrieve the adjacency matrix representation of each graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qsOw9zFwrxDe"
   },
   "outputs": [],
   "source": [
    "# convert graphs from StellarGraph format to numpy adj matrices\n",
    "adjs = [graph.to_adjacency_matrix().A for graph in graphs]\n",
    "# convert labes fom Pandas.Series to numpy array\n",
    "labels = graph_labels.to_numpy(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6S5M5mL2t-ik"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "metrics = []\n",
    "for adj in adjs:\n",
    "    G = nx.from_numpy_matrix(adj)\n",
    "    # basic properties\n",
    "    num_edges = G.number_of_edges()\n",
    "    # clustering measures\n",
    "    cc = nx.average_clustering(G)\n",
    "    # measure of efficiency\n",
    "    eff = nx.global_efficiency(G)\n",
    "\n",
    "    metrics.append([num_edges, cc, eff])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_a5CiZKL4vW"
   },
   "source": [
    "We can now exploit scikit-learn utilities to create a train and test set. In our experiments, we will be using 70% of the dataset as training set and the remaining as testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NRrNPqOxu7eY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMIF1weiMO0F"
   },
   "source": [
    "As commonly done in many Machine Learning workflows, we preprocess features to have zero mean and unit standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9qUjNhPru6ni"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqaZzejRMdmu"
   },
   "source": [
    "It's now time for training a proper algorithm. We chose a support vector machine for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3A6_fh0OV9x",
    "outputId": "6297d8fe-3cc9-435b-e8fe-b50425aaee24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7455089820359282\n",
      "Precision 0.7709251101321586\n",
      "Recall 0.8413461538461539\n",
      "F1-score 0.8045977011494253\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "print('Accuracy', accuracy_score(y_test,y_pred))\n",
    "print('Precision', precision_score(y_test,y_pred))\n",
    "print('Recall', recall_score(y_test,y_pred))\n",
    "print('F1-score', f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBVKcDWHeGoR"
   },
   "source": [
    "# Supervised graph representation learning using Graph ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb6FvAQ3eUNs"
   },
   "source": [
    "In this notebook we will be performing supervised graph representation learning using Deep Graph ConvNet as encoder.\n",
    "\n",
    "The model embeds a graph by using stacked Graph ConvNet layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHU1UGiHfw1e"
   },
   "source": [
    "In this demo, we will be using the PROTEINS dataset, already integrated in StellarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "_8SDtHy1PfNx",
    "outputId": "aa1f8875-ab8d-42b6-eadc-8084f1796cc1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Each graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dataset = datasets.PROTEINS()\n",
    "display(HTML(dataset.description))\n",
    "graphs, graph_labels = dataset.load()\n",
    "\n",
    "labels = graph_labels.to_numpy(dtype=int)\n",
    "\n",
    "# necessary for converting default string labels to int\n",
    "graph_labels = pd.get_dummies(graph_labels, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uEUzYBIM6cK"
   },
   "source": [
    "StellarGraph we are using for building the model, uses tf.Keras as backend. According to its specific, we need a data generator for feeding the model. For supervised graph classification, we create an instance of StellarGraph's PaddedGraphGenerator class. This generator supplies the features arrays and the adjacency matrices to a mini-batch Keras graph classification model. Differences in the number of nodes are resolved by padding each batch of features and adjacency matrices, and supplying a boolean mask indicating which are valid and which are padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "i34QgSA_P_sM"
   },
   "outputs": [],
   "source": [
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "generator = PaddedGraphGenerator(graphs=graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YepZYuk_NWWT"
   },
   "source": [
    "Now we are ready for actually create the model. The GCN layers will be created and stacked togheter through StellarGraph's utility function. This _backbone_ will be then concateneted to 1D Convolutional layers and Fully connected layers using tf.Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qF8DHIalQuIW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jhurtado/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/jhurtado/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "from stellargraph.layer import DeepGraphCNN\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "nrows = 35  # the number of rows for the output tensor\n",
    "layer_dims = [32, 32, 32, 1]\n",
    "\n",
    "dgcnn_model = DeepGraphCNN(\n",
    "    layer_sizes=layer_dims,\n",
    "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
    "    k=nrows,\n",
    "    bias=False,\n",
    "    generator=generator,\n",
    ")\n",
    "gnn_inp, gnn_out = dgcnn_model.in_out_tensors()\n",
    "\n",
    "\n",
    "x_out = Conv1D(filters=16, kernel_size=sum(layer_dims), strides=sum(layer_dims))(gnn_out)\n",
    "x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "\n",
    "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "\n",
    "x_out = Flatten()(x_out)\n",
    "\n",
    "x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
    "x_out = Dropout(rate=0.5)(x_out)\n",
    "\n",
    "predictions = Dense(units=1, activation=\"sigmoid\")(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOj3TjPIN4ev"
   },
   "source": [
    "Let's now compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "clWqCmfLJjBF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=gnn_inp, outputs=predictions)\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss=binary_crossentropy, metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZnhaSMDN9ii"
   },
   "source": [
    "We use 70% of the dataset for training and the remaining for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j3Hr6_FyJ5m4"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    graph_labels, test_size=.3, stratify=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p9_2ybPqJ-3B"
   },
   "outputs": [],
   "source": [
    "gen = PaddedGraphGenerator(graphs=graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=train_graphs.values,\n",
    "    symmetric_normalization=False,\n",
    "    batch_size=50,\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=test_graphs.values,\n",
    "    symmetric_normalization=False,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCNr8_IsOIbQ"
   },
   "source": [
    "It's now time for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3b2BNJUKKas",
    "outputId": "e3c8b8e7-0beb-4479-9829-793f781e1a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 3s 111ms/step - loss: 0.5937 - acc: 0.6983 - val_loss: 0.6169 - val_acc: 0.6886\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.5683 - acc: 0.7291 - val_loss: 0.6000 - val_acc: 0.6946\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.5572 - acc: 0.7407 - val_loss: 0.5864 - val_acc: 0.7006\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.5380 - acc: 0.7420 - val_loss: 0.5843 - val_acc: 0.7036\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.5321 - acc: 0.7471 - val_loss: 0.5829 - val_acc: 0.7096\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.5237 - acc: 0.7497 - val_loss: 0.5717 - val_acc: 0.7066\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.5163 - acc: 0.7535 - val_loss: 0.5838 - val_acc: 0.7036\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.5303 - acc: 0.7445 - val_loss: 0.5733 - val_acc: 0.7036\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.5274 - acc: 0.7587 - val_loss: 0.6056 - val_acc: 0.6826\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.5261 - acc: 0.7522 - val_loss: 0.5760 - val_acc: 0.7126\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.5207 - acc: 0.7599 - val_loss: 0.5688 - val_acc: 0.7006\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.5093 - acc: 0.7587 - val_loss: 0.5877 - val_acc: 0.7036\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 0.5104 - acc: 0.7599 - val_loss: 0.5793 - val_acc: 0.7216\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.5125 - acc: 0.7599 - val_loss: 0.5828 - val_acc: 0.6916\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.5078 - acc: 0.7548 - val_loss: 0.5829 - val_acc: 0.6796\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.5230 - acc: 0.7471 - val_loss: 0.5689 - val_acc: 0.7126\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.5180 - acc: 0.7381 - val_loss: 0.5620 - val_acc: 0.7066\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.5229 - acc: 0.7548 - val_loss: 0.5673 - val_acc: 0.7126\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.5001 - acc: 0.7741 - val_loss: 0.5665 - val_acc: 0.7006\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.4838 - acc: 0.7677 - val_loss: 0.5980 - val_acc: 0.6826\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.4917 - acc: 0.7664 - val_loss: 0.5617 - val_acc: 0.7186\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.4976 - acc: 0.7728 - val_loss: 0.5667 - val_acc: 0.7216\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.4887 - acc: 0.7728 - val_loss: 0.5574 - val_acc: 0.7275\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.4934 - acc: 0.7779 - val_loss: 0.5626 - val_acc: 0.7036\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.4747 - acc: 0.7779 - val_loss: 0.5587 - val_acc: 0.7036\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.4752 - acc: 0.7908 - val_loss: 0.5675 - val_acc: 0.7186\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.4775 - acc: 0.7792 - val_loss: 0.5610 - val_acc: 0.7246\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.4790 - acc: 0.7895 - val_loss: 0.5660 - val_acc: 0.7186\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.4780 - acc: 0.7882 - val_loss: 0.5587 - val_acc: 0.7156\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.4673 - acc: 0.7715 - val_loss: 0.5756 - val_acc: 0.7156\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.4556 - acc: 0.8023 - val_loss: 0.5633 - val_acc: 0.7096\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.4635 - acc: 0.7856 - val_loss: 0.5609 - val_acc: 0.7156\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.4639 - acc: 0.7946 - val_loss: 0.5635 - val_acc: 0.7036\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.4698 - acc: 0.7972 - val_loss: 0.5593 - val_acc: 0.7186\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.4513 - acc: 0.7972 - val_loss: 0.5807 - val_acc: 0.7365\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.4475 - acc: 0.7959 - val_loss: 0.5605 - val_acc: 0.7186\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.4405 - acc: 0.7997 - val_loss: 0.5632 - val_acc: 0.7126\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.4343 - acc: 0.8100 - val_loss: 0.5929 - val_acc: 0.7066\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.4629 - acc: 0.7856 - val_loss: 0.5694 - val_acc: 0.7096\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.4347 - acc: 0.8087 - val_loss: 0.5712 - val_acc: 0.7126\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.4311 - acc: 0.8036 - val_loss: 0.5717 - val_acc: 0.7186\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.4345 - acc: 0.7997 - val_loss: 0.5671 - val_acc: 0.7216\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.4230 - acc: 0.8254 - val_loss: 0.5773 - val_acc: 0.7246\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.4253 - acc: 0.8062 - val_loss: 0.5797 - val_acc: 0.7156\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 0.4042 - acc: 0.8383 - val_loss: 0.5756 - val_acc: 0.7305\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.4010 - acc: 0.8177 - val_loss: 0.6090 - val_acc: 0.7036\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.4223 - acc: 0.8049 - val_loss: 0.5755 - val_acc: 0.7485\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.4021 - acc: 0.8293 - val_loss: 0.6019 - val_acc: 0.7066\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4175 - acc: 0.8177 - val_loss: 0.5914 - val_acc: 0.7246\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.4044 - acc: 0.8241 - val_loss: 0.5895 - val_acc: 0.7096\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.4106 - acc: 0.8293 - val_loss: 0.6071 - val_acc: 0.7246\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.3924 - acc: 0.8241 - val_loss: 0.5900 - val_acc: 0.7126\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.3890 - acc: 0.8126 - val_loss: 0.5900 - val_acc: 0.7126\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.4114 - acc: 0.8190 - val_loss: 0.5956 - val_acc: 0.7246\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.4105 - acc: 0.8190 - val_loss: 0.6149 - val_acc: 0.7156\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.4022 - acc: 0.8254 - val_loss: 0.5980 - val_acc: 0.7096\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.3717 - acc: 0.8472 - val_loss: 0.6004 - val_acc: 0.7096\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.3703 - acc: 0.8460 - val_loss: 0.6162 - val_acc: 0.7156\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.3789 - acc: 0.8370 - val_loss: 0.6199 - val_acc: 0.7216\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.3801 - acc: 0.8383 - val_loss: 0.6048 - val_acc: 0.7275\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.3843 - acc: 0.8357 - val_loss: 0.6227 - val_acc: 0.7275\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.3963 - acc: 0.8434 - val_loss: 0.6058 - val_acc: 0.7216\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.3641 - acc: 0.8447 - val_loss: 0.6169 - val_acc: 0.7096\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.3594 - acc: 0.8395 - val_loss: 0.5955 - val_acc: 0.7275\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.3570 - acc: 0.8395 - val_loss: 0.6351 - val_acc: 0.7335\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.3566 - acc: 0.8408 - val_loss: 0.6212 - val_acc: 0.7275\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.3408 - acc: 0.8575 - val_loss: 0.6242 - val_acc: 0.7275\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.3731 - acc: 0.8395 - val_loss: 0.6179 - val_acc: 0.7156\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.3488 - acc: 0.8575 - val_loss: 0.6300 - val_acc: 0.7156\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.3389 - acc: 0.8614 - val_loss: 0.6538 - val_acc: 0.7096\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.3390 - acc: 0.8549 - val_loss: 0.6480 - val_acc: 0.7126\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.3338 - acc: 0.8614 - val_loss: 0.6609 - val_acc: 0.6976\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.3449 - acc: 0.8562 - val_loss: 0.6719 - val_acc: 0.6976\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.3395 - acc: 0.8614 - val_loss: 0.6875 - val_acc: 0.7066\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.3285 - acc: 0.8614 - val_loss: 0.7012 - val_acc: 0.7156\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.3235 - acc: 0.8601 - val_loss: 0.6677 - val_acc: 0.7275\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.3438 - acc: 0.8626 - val_loss: 0.7092 - val_acc: 0.7425\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.3162 - acc: 0.8524 - val_loss: 0.7090 - val_acc: 0.7066\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.3139 - acc: 0.8652 - val_loss: 0.6829 - val_acc: 0.7216\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.2908 - acc: 0.8755 - val_loss: 0.6652 - val_acc: 0.7066\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.3074 - acc: 0.8768 - val_loss: 0.7038 - val_acc: 0.7305\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.3130 - acc: 0.8819 - val_loss: 0.7095 - val_acc: 0.7126\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.3090 - acc: 0.8716 - val_loss: 0.7239 - val_acc: 0.6946\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.3325 - acc: 0.8691 - val_loss: 0.6670 - val_acc: 0.7365\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.3354 - acc: 0.8537 - val_loss: 0.6576 - val_acc: 0.7246\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.2988 - acc: 0.8832 - val_loss: 0.7288 - val_acc: 0.7066\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.2940 - acc: 0.8652 - val_loss: 0.7420 - val_acc: 0.6946\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.2810 - acc: 0.8793 - val_loss: 0.7590 - val_acc: 0.7066\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.3265 - acc: 0.8626 - val_loss: 0.7169 - val_acc: 0.7126\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.3195 - acc: 0.8691 - val_loss: 0.6945 - val_acc: 0.7036\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.3246 - acc: 0.8742 - val_loss: 0.7083 - val_acc: 0.6916\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.3222 - acc: 0.8742 - val_loss: 0.7223 - val_acc: 0.7216\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.3222 - acc: 0.8549 - val_loss: 0.6772 - val_acc: 0.7096\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.3031 - acc: 0.8780 - val_loss: 0.7849 - val_acc: 0.7156\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.3084 - acc: 0.8896 - val_loss: 0.7388 - val_acc: 0.7246\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.2661 - acc: 0.8793 - val_loss: 0.7305 - val_acc: 0.7246\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.2879 - acc: 0.8870 - val_loss: 0.7569 - val_acc: 0.7126\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.2851 - acc: 0.8780 - val_loss: 0.6900 - val_acc: 0.7096\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.2788 - acc: 0.8883 - val_loss: 0.7806 - val_acc: 0.7006\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.2696 - acc: 0.8896 - val_loss: 0.8016 - val_acc: 0.7066\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gdPBykJ4KPrV"
   },
   "outputs": [],
   "source": [
    "# https://stellargraph.readthedocs.io/en/stable/demos/graph-classification/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1lM0v05_zJe"
   },
   "source": [
    "## Supervised node representation learning using GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "gERK1Zen_xL7",
    "outputId": "5959207e-3139-4262-fdbf-502d255bc826"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dataset = datasets.Cora()\n",
    "display(HTML(dataset.description))\n",
    "G, nodes = dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrkhfxtenQ4i"
   },
   "source": [
    "Let's split the dataset into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RZsS_u7v_5vc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_nodes, test_nodes = train_test_split(\n",
    "    nodes, train_size=0.1, test_size=None, stratify=nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm4-Me5GnVce"
   },
   "source": [
    "Since we are performing a categorical classification, it is useful to represent each categorical label in its one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dP-sXgekAFOY"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "label_encoding = preprocessing.LabelBinarizer()\n",
    "train_labels = label_encoding.fit_transform(train_nodes)\n",
    "test_labels = label_encoding.transform(test_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJSpM4cnnfUN"
   },
   "source": [
    "It's now time for creating the mdoel. It will be composed by two GraphSAGE layers followed by a Dense layer with softmax activation for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BP5G49akANxy"
   },
   "outputs": [],
   "source": [
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "batchsize = 50\n",
    "n_samples = [10, 5, 7]\n",
    "generator = GraphSAGENodeGenerator(G, batchsize, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qkgF2VvWAlct"
   },
   "outputs": [],
   "source": [
    "from stellargraph.layer import GraphSAGE\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "graphsage_model = GraphSAGE(\n",
    "    layer_sizes=[32, 32, 16], generator=generator, bias=True, dropout=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S_g_yOhjAovl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhurtado/.local/lib/python3.8/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gnn_inp, gnn_out = graphsage_model.in_out_tensors()\n",
    "outputs = Dense(units=train_labels.shape[1], activation=\"softmax\")(gnn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HeMlOuDnA9B_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhurtado/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Model(inputs=gnn_inp, outputs=outputs)\n",
    "model.compile(optimizer=Adam(lr=0.003), loss=categorical_crossentropy, metrics=[\"acc\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqF4EWFbnwU9"
   },
   "source": [
    "We will use the flow function of the generator for feeding the model with the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "x-5xmzRqBDCX"
   },
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_nodes.index, train_labels, shuffle=True)\n",
    "test_gen = generator.flow(test_nodes.index, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "952-5V6Xn45o"
   },
   "source": [
    "Finally, let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sS3vnQ_HBZxK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 - 10s - loss: 1.9363 - acc: 0.1926 - val_loss: 1.8286 - val_acc: 0.3023 - 10s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "6/6 - 9s - loss: 1.8533 - acc: 0.2963 - val_loss: 1.8108 - val_acc: 0.3023 - 9s/epoch - 2s/step\n",
      "Epoch 3/20\n",
      "6/6 - 9s - loss: 1.8107 - acc: 0.3074 - val_loss: 1.7773 - val_acc: 0.3023 - 9s/epoch - 1s/step\n",
      "Epoch 4/20\n",
      "6/6 - 9s - loss: 1.7594 - acc: 0.3111 - val_loss: 1.7000 - val_acc: 0.3072 - 9s/epoch - 1s/step\n",
      "Epoch 5/20\n",
      "6/6 - 9s - loss: 1.7010 - acc: 0.3741 - val_loss: 1.5865 - val_acc: 0.3876 - 9s/epoch - 1s/step\n",
      "Epoch 6/20\n",
      "6/6 - 9s - loss: 1.6181 - acc: 0.4407 - val_loss: 1.4922 - val_acc: 0.5197 - 9s/epoch - 1s/step\n",
      "Epoch 7/20\n",
      "6/6 - 9s - loss: 1.5648 - acc: 0.5556 - val_loss: 1.4300 - val_acc: 0.5923 - 9s/epoch - 2s/step\n",
      "Epoch 8/20\n",
      "6/6 - 9s - loss: 1.4709 - acc: 0.6185 - val_loss: 1.3781 - val_acc: 0.6169 - 9s/epoch - 2s/step\n",
      "Epoch 9/20\n",
      "6/6 - 9s - loss: 1.4167 - acc: 0.6333 - val_loss: 1.3270 - val_acc: 0.6345 - 9s/epoch - 1s/step\n",
      "Epoch 10/20\n",
      "6/6 - 9s - loss: 1.3500 - acc: 0.6852 - val_loss: 1.2665 - val_acc: 0.6747 - 9s/epoch - 1s/step\n",
      "Epoch 11/20\n",
      "6/6 - 9s - loss: 1.2780 - acc: 0.7148 - val_loss: 1.2232 - val_acc: 0.7051 - 9s/epoch - 1s/step\n",
      "Epoch 12/20\n",
      "6/6 - 9s - loss: 1.2269 - acc: 0.7593 - val_loss: 1.1792 - val_acc: 0.7285 - 9s/epoch - 2s/step\n",
      "Epoch 13/20\n",
      "6/6 - 9s - loss: 1.1580 - acc: 0.7704 - val_loss: 1.1419 - val_acc: 0.7248 - 9s/epoch - 2s/step\n",
      "Epoch 14/20\n",
      "6/6 - 9s - loss: 1.1044 - acc: 0.7852 - val_loss: 1.0987 - val_acc: 0.7494 - 9s/epoch - 2s/step\n",
      "Epoch 15/20\n",
      "6/6 - 9s - loss: 1.0618 - acc: 0.8111 - val_loss: 1.0805 - val_acc: 0.7465 - 9s/epoch - 2s/step\n",
      "Epoch 16/20\n",
      "6/6 - 9s - loss: 1.0155 - acc: 0.8593 - val_loss: 1.0520 - val_acc: 0.7662 - 9s/epoch - 1s/step\n",
      "Epoch 17/20\n",
      "6/6 - 9s - loss: 0.9788 - acc: 0.8481 - val_loss: 1.0164 - val_acc: 0.7687 - 9s/epoch - 1s/step\n",
      "Epoch 18/20\n",
      "6/6 - 9s - loss: 0.9628 - acc: 0.8519 - val_loss: 0.9907 - val_acc: 0.7765 - 9s/epoch - 2s/step\n",
      "Epoch 19/20\n",
      "6/6 - 9s - loss: 0.9025 - acc: 0.8963 - val_loss: 0.9618 - val_acc: 0.7863 - 9s/epoch - 2s/step\n",
      "Epoch 20/20\n",
      "6/6 - 9s - loss: 0.8679 - acc: 0.8667 - val_loss: 0.9512 - val_acc: 0.7834 - 9s/epoch - 2s/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGJXHirZBcuL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFZJ2OJcoQgi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Supervised_GraphML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b632d4f284bbe77588fa149b6cb67b77182ce71d42208176a727bc04a09d6e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
