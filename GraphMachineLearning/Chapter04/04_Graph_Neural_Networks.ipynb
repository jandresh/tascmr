{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8HAOWyDgPiH"
   },
   "source": [
    "# Shallow methods for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6w1KA97gXao"
   },
   "source": [
    "In this notebook we will exploring a very naive (yet powerful) approach for solving graph-based supervised machine learning. The idea rely on the classic machine learning approach of handcrafted feature extraction.\n",
    "\n",
    "In Chapter 1 you learned how local and global graph properties can be extracted from graphs. Those properties represent the graph itself and bring important informations which can be useful for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5k3sYIRJpMgb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install stellargraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWL_AuChPcYS"
   },
   "source": [
    "In this demo, we will be using the PROTEINS dataset, already integrated in StellarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "gS5B47T2gWll",
    "outputId": "4020adc2-75b7-4aa5-b480-24d2693a8a74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 11:38:05.675992: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-22 11:38:05.676022: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-22 11:38:06.791433: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-22 11:38:06.791608: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-22 11:38:06.791620: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-22 11:38:06.791637: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (21f317809674): /proc/driver/nvidia/version does not exist\n",
      "2023-03-22 11:38:06.791852: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-22 11:38:06.792098: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Each graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dataset = datasets.PROTEINS()\n",
    "display(HTML(dataset.description))\n",
    "graphs, graph_labels = dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDlUMUUFLrjh"
   },
   "source": [
    "To compute the graph metrics, one way is to retrieve the adjacency matrix representation of each graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qsOw9zFwrxDe"
   },
   "outputs": [],
   "source": [
    "# convert graphs from StellarGraph format to numpy adj matrices\n",
    "adjs = [graph.to_adjacency_matrix().A for graph in graphs]\n",
    "# convert labes fom Pandas.Series to numpy array\n",
    "labels = graph_labels.to_numpy(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6S5M5mL2t-ik"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "metrics = []\n",
    "for adj in adjs:\n",
    "    G = nx.from_numpy_matrix(adj)\n",
    "    # basic properties\n",
    "    num_edges = G.number_of_edges()\n",
    "    # clustering measures\n",
    "    cc = nx.average_clustering(G)\n",
    "    # measure of efficiency\n",
    "    eff = nx.global_efficiency(G)\n",
    "\n",
    "    metrics.append([num_edges, cc, eff])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_a5CiZKL4vW"
   },
   "source": [
    "We can now exploit scikit-learn utilities to create a train and test set. In our experiments, we will be using 70% of the dataset as training set and the remaining as testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NRrNPqOxu7eY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMIF1weiMO0F"
   },
   "source": [
    "As commonly done in many Machine Learning workflows, we preprocess features to have zero mean and unit standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9qUjNhPru6ni"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqaZzejRMdmu"
   },
   "source": [
    "It's now time for training a proper algorithm. We chose a support vector machine for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3A6_fh0OV9x",
    "outputId": "6297d8fe-3cc9-435b-e8fe-b50425aaee24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7455089820359282\n",
      "Precision 0.7709251101321586\n",
      "Recall 0.8413461538461539\n",
      "F1-score 0.8045977011494253\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "print('Accuracy', accuracy_score(y_test,y_pred))\n",
    "print('Precision', precision_score(y_test,y_pred))\n",
    "print('Recall', recall_score(y_test,y_pred))\n",
    "print('F1-score', f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBVKcDWHeGoR"
   },
   "source": [
    "# Supervised graph representation learning using Graph ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb6FvAQ3eUNs"
   },
   "source": [
    "In this notebook we will be performing supervised graph representation learning using Deep Graph ConvNet as encoder.\n",
    "\n",
    "The model embeds a graph by using stacked Graph ConvNet layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHU1UGiHfw1e"
   },
   "source": [
    "In this demo, we will be using the PROTEINS dataset, already integrated in StellarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "_8SDtHy1PfNx",
    "outputId": "aa1f8875-ab8d-42b6-eadc-8084f1796cc1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Each graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dataset = datasets.PROTEINS()\n",
    "display(HTML(dataset.description))\n",
    "graphs, graph_labels = dataset.load()\n",
    "\n",
    "labels = graph_labels.to_numpy(dtype=int)\n",
    "\n",
    "# necessary for converting default string labels to int\n",
    "graph_labels = pd.get_dummies(graph_labels, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uEUzYBIM6cK"
   },
   "source": [
    "StellarGraph we are using for building the model, uses tf.Keras as backend. According to its specific, we need a data generator for feeding the model. For supervised graph classification, we create an instance of StellarGraph's PaddedGraphGenerator class. This generator supplies the features arrays and the adjacency matrices to a mini-batch Keras graph classification model. Differences in the number of nodes are resolved by padding each batch of features and adjacency matrices, and supplying a boolean mask indicating which are valid and which are padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "i34QgSA_P_sM"
   },
   "outputs": [],
   "source": [
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "generator = PaddedGraphGenerator(graphs=graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YepZYuk_NWWT"
   },
   "source": [
    "Now we are ready for actually create the model. The GCN layers will be created and stacked togheter through StellarGraph's utility function. This _backbone_ will be then concateneted to 1D Convolutional layers and Fully connected layers using tf.Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qF8DHIalQuIW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "from stellargraph.layer import DeepGraphCNN\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "nrows = 35  # the number of rows for the output tensor\n",
    "layer_dims = [32, 32, 32, 1]\n",
    "\n",
    "dgcnn_model = DeepGraphCNN(\n",
    "    layer_sizes=layer_dims,\n",
    "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
    "    k=nrows,\n",
    "    bias=False,\n",
    "    generator=generator,\n",
    ")\n",
    "gnn_inp, gnn_out = dgcnn_model.in_out_tensors()\n",
    "\n",
    "\n",
    "x_out = Conv1D(filters=16, kernel_size=sum(layer_dims), strides=sum(layer_dims))(gnn_out)\n",
    "x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "\n",
    "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "\n",
    "x_out = Flatten()(x_out)\n",
    "\n",
    "x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
    "x_out = Dropout(rate=0.5)(x_out)\n",
    "\n",
    "predictions = Dense(units=1, activation=\"sigmoid\")(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOj3TjPIN4ev"
   },
   "source": [
    "Let's now compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "clWqCmfLJjBF"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=gnn_inp, outputs=predictions)\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss=binary_crossentropy, metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZnhaSMDN9ii"
   },
   "source": [
    "We use 70% of the dataset for training and the remaining for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j3Hr6_FyJ5m4"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    graph_labels, test_size=.3, stratify=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p9_2ybPqJ-3B"
   },
   "outputs": [],
   "source": [
    "gen = PaddedGraphGenerator(graphs=graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=train_graphs.values,\n",
    "    symmetric_normalization=False,\n",
    "    batch_size=50,\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=test_graphs.values,\n",
    "    symmetric_normalization=False,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCNr8_IsOIbQ"
   },
   "source": [
    "It's now time for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3b2BNJUKKas",
    "outputId": "e3c8b8e7-0beb-4479-9829-793f781e1a03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 11:38:28.500336: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-03-22 11:38:28.519094: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/sort_pooling/map/while/gradients/model/sort_pooling/map/while/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/sort_pooling/map/while/gradients/model/sort_pooling/map/while/GatherV2_grad/Reshape:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/sort_pooling/map/while/gradients/model/sort_pooling/map/while/GatherV2_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/16 [>.............................] - ETA: 19s - loss: 0.7161 - acc: 0.4800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 11:38:29.904274: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 76880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 117ms/step - loss: 0.6968 - acc: 0.5239 - val_loss: 0.6383 - val_acc: 0.6617\n",
      "Epoch 2/100\n",
      " 5/16 [========>.....................] - ETA: 1s - loss: 0.6406 - acc: 0.6329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 11:38:31.815607: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 76880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 82ms/step - loss: 0.6314 - acc: 0.6447 - val_loss: 0.6189 - val_acc: 0.6677\n",
      "Epoch 3/100\n",
      " 8/16 [==============>...............] - ETA: 0s - loss: 0.5990 - acc: 0.6590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 11:38:33.269068: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 76880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 82ms/step - loss: 0.6064 - acc: 0.6597 - val_loss: 0.6082 - val_acc: 0.7036\n",
      "Epoch 4/100\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 0.5922 - acc: 0.6893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 11:38:34.777061: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 76880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 81ms/step - loss: 0.5928 - acc: 0.6909 - val_loss: 0.5990 - val_acc: 0.7096\n",
      "Epoch 5/100\n",
      " 3/16 [====>.........................] - ETA: 2s - loss: 0.6158 - acc: 0.7089"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 11:38:35.663251: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 76880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 78ms/step - loss: 0.5939 - acc: 0.7222 - val_loss: 0.5930 - val_acc: 0.7246\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.5779 - acc: 0.7376 - val_loss: 0.5878 - val_acc: 0.7156\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.6037 - acc: 0.7132 - val_loss: 0.5862 - val_acc: 0.7186\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.5792 - acc: 0.7263 - val_loss: 0.5832 - val_acc: 0.7156\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.5843 - acc: 0.7186 - val_loss: 0.5827 - val_acc: 0.7126\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.5669 - acc: 0.7191 - val_loss: 0.5815 - val_acc: 0.7126\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.5704 - acc: 0.7310 - val_loss: 0.5792 - val_acc: 0.7216\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.5808 - acc: 0.7070 - val_loss: 0.5791 - val_acc: 0.7216\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.5573 - acc: 0.7391 - val_loss: 0.5769 - val_acc: 0.7156\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.5473 - acc: 0.7571 - val_loss: 0.5756 - val_acc: 0.7156\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.5765 - acc: 0.7251 - val_loss: 0.5751 - val_acc: 0.7216\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.5587 - acc: 0.7252 - val_loss: 0.5770 - val_acc: 0.7216\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.5696 - acc: 0.7346 - val_loss: 0.5727 - val_acc: 0.7216\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.5550 - acc: 0.7263 - val_loss: 0.5726 - val_acc: 0.7216\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.5696 - acc: 0.7151 - val_loss: 0.5739 - val_acc: 0.7186\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.5788 - acc: 0.7217 - val_loss: 0.5716 - val_acc: 0.7216\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5915 - acc: 0.7047 - val_loss: 0.5711 - val_acc: 0.7186\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5765 - acc: 0.7169 - val_loss: 0.5710 - val_acc: 0.7186\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.5638 - acc: 0.7362 - val_loss: 0.5692 - val_acc: 0.7216\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.5364 - acc: 0.7442 - val_loss: 0.5703 - val_acc: 0.7216\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5448 - acc: 0.7218 - val_loss: 0.5670 - val_acc: 0.7186\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.5652 - acc: 0.7282 - val_loss: 0.5679 - val_acc: 0.7186\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.5608 - acc: 0.7357 - val_loss: 0.5656 - val_acc: 0.7156\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.5557 - acc: 0.7300 - val_loss: 0.5658 - val_acc: 0.7156\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5549 - acc: 0.7356 - val_loss: 0.5646 - val_acc: 0.7186\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5403 - acc: 0.7417 - val_loss: 0.5652 - val_acc: 0.7156\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.5389 - acc: 0.7326 - val_loss: 0.5633 - val_acc: 0.7186\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 0.5396 - acc: 0.7442 - val_loss: 0.5624 - val_acc: 0.7186\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.5279 - acc: 0.7553 - val_loss: 0.5618 - val_acc: 0.7186\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.5615 - acc: 0.7065 - val_loss: 0.5612 - val_acc: 0.7216\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.5506 - acc: 0.7348 - val_loss: 0.5602 - val_acc: 0.7275\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.5435 - acc: 0.7224 - val_loss: 0.5609 - val_acc: 0.7216\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.5516 - acc: 0.7401 - val_loss: 0.5594 - val_acc: 0.7246\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5546 - acc: 0.7097 - val_loss: 0.5580 - val_acc: 0.7335\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 0.5338 - acc: 0.7389 - val_loss: 0.5595 - val_acc: 0.7216\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.5641 - acc: 0.7034 - val_loss: 0.5569 - val_acc: 0.7275\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.5426 - acc: 0.7228 - val_loss: 0.5567 - val_acc: 0.7305\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.5418 - acc: 0.7183 - val_loss: 0.5602 - val_acc: 0.7216\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 1s 97ms/step - loss: 0.5373 - acc: 0.7469 - val_loss: 0.5561 - val_acc: 0.7335\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.5484 - acc: 0.7456 - val_loss: 0.5585 - val_acc: 0.7246\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 0.5378 - acc: 0.7441 - val_loss: 0.5550 - val_acc: 0.7365\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.5207 - acc: 0.7580 - val_loss: 0.5545 - val_acc: 0.7305\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 0.5621 - acc: 0.7253 - val_loss: 0.5546 - val_acc: 0.7275\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.5264 - acc: 0.7522 - val_loss: 0.5532 - val_acc: 0.7365\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 0.5273 - acc: 0.7316 - val_loss: 0.5527 - val_acc: 0.7335\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.5294 - acc: 0.7529 - val_loss: 0.5524 - val_acc: 0.7365\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.5256 - acc: 0.7443 - val_loss: 0.5519 - val_acc: 0.7365\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 2s 77ms/step - loss: 0.5289 - acc: 0.7393 - val_loss: 0.5513 - val_acc: 0.7365\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 2s 78ms/step - loss: 0.5309 - acc: 0.7478 - val_loss: 0.5512 - val_acc: 0.7365\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5332 - acc: 0.7362 - val_loss: 0.5506 - val_acc: 0.7275\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.5162 - acc: 0.7485 - val_loss: 0.5519 - val_acc: 0.7365\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.5281 - acc: 0.7516 - val_loss: 0.5497 - val_acc: 0.7335\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5545 - acc: 0.7112 - val_loss: 0.5517 - val_acc: 0.7335\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 0.5152 - acc: 0.7704 - val_loss: 0.5472 - val_acc: 0.7395\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 2s 79ms/step - loss: 0.5376 - acc: 0.7400 - val_loss: 0.5503 - val_acc: 0.7275\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 0.5528 - acc: 0.7255 - val_loss: 0.5464 - val_acc: 0.7395\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.5529 - acc: 0.7237 - val_loss: 0.5456 - val_acc: 0.7395\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.5175 - acc: 0.7499 - val_loss: 0.5472 - val_acc: 0.7395\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.5107 - acc: 0.7495 - val_loss: 0.5477 - val_acc: 0.7395\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5585 - acc: 0.7026 - val_loss: 0.5447 - val_acc: 0.7485\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.5452 - acc: 0.7219 - val_loss: 0.5448 - val_acc: 0.7425\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.5230 - acc: 0.7241 - val_loss: 0.5522 - val_acc: 0.7275\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.5301 - acc: 0.7506 - val_loss: 0.5439 - val_acc: 0.7455\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.5062 - acc: 0.7457 - val_loss: 0.5449 - val_acc: 0.7425\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5330 - acc: 0.7360 - val_loss: 0.5456 - val_acc: 0.7365\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5173 - acc: 0.7549 - val_loss: 0.5436 - val_acc: 0.7395\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.5414 - acc: 0.7500 - val_loss: 0.5427 - val_acc: 0.7425\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.5290 - acc: 0.7292 - val_loss: 0.5422 - val_acc: 0.7425\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.5167 - acc: 0.7343 - val_loss: 0.5441 - val_acc: 0.7335\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5093 - acc: 0.7323 - val_loss: 0.5418 - val_acc: 0.7425\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.5062 - acc: 0.7597 - val_loss: 0.5420 - val_acc: 0.7335\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5035 - acc: 0.7483 - val_loss: 0.5394 - val_acc: 0.7395\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.5333 - acc: 0.7360 - val_loss: 0.5501 - val_acc: 0.7335\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.5159 - acc: 0.7425 - val_loss: 0.5382 - val_acc: 0.7425\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5333 - acc: 0.7380 - val_loss: 0.5409 - val_acc: 0.7305\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.5075 - acc: 0.7553 - val_loss: 0.5376 - val_acc: 0.7365\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.5363 - acc: 0.7265 - val_loss: 0.5446 - val_acc: 0.7365\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5175 - acc: 0.7478 - val_loss: 0.5362 - val_acc: 0.7425\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.5236 - acc: 0.7246 - val_loss: 0.5371 - val_acc: 0.7455\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5144 - acc: 0.7356 - val_loss: 0.5370 - val_acc: 0.7425\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.5166 - acc: 0.7302 - val_loss: 0.5367 - val_acc: 0.7455\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 0.5035 - acc: 0.7503 - val_loss: 0.5352 - val_acc: 0.7425\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5263 - acc: 0.7402 - val_loss: 0.5429 - val_acc: 0.7455\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5005 - acc: 0.7569 - val_loss: 0.5337 - val_acc: 0.7485\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.5055 - acc: 0.7573 - val_loss: 0.5354 - val_acc: 0.7395\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.4919 - acc: 0.7670 - val_loss: 0.5328 - val_acc: 0.7455\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.5095 - acc: 0.7341 - val_loss: 0.5353 - val_acc: 0.7425\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.4992 - acc: 0.7637 - val_loss: 0.5351 - val_acc: 0.7425\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.4770 - acc: 0.7801 - val_loss: 0.5317 - val_acc: 0.7485\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5077 - acc: 0.7500 - val_loss: 0.5309 - val_acc: 0.7485\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 0.5084 - acc: 0.7443 - val_loss: 0.5300 - val_acc: 0.7545\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5125 - acc: 0.7384 - val_loss: 0.5362 - val_acc: 0.7335\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5096 - acc: 0.7386 - val_loss: 0.5322 - val_acc: 0.7395\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.5047 - acc: 0.7283 - val_loss: 0.5293 - val_acc: 0.7515\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.5072 - acc: 0.7439 - val_loss: 0.5330 - val_acc: 0.7395\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.5044 - acc: 0.7508 - val_loss: 0.5315 - val_acc: 0.7425\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gdPBykJ4KPrV"
   },
   "outputs": [],
   "source": [
    "# https://stellargraph.readthedocs.io/en/stable/demos/graph-classification/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1lM0v05_zJe"
   },
   "source": [
    "## Supervised node representation learning using GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "gERK1Zen_xL7",
    "outputId": "5959207e-3139-4262-fdbf-502d255bc826"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dataset = datasets.Cora()\n",
    "display(HTML(dataset.description))\n",
    "G, nodes = dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrkhfxtenQ4i"
   },
   "source": [
    "Let's split the dataset into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RZsS_u7v_5vc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_nodes, test_nodes = train_test_split(\n",
    "    nodes, train_size=0.1, test_size=None, stratify=nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm4-Me5GnVce"
   },
   "source": [
    "Since we are performing a categorical classification, it is useful to represent each categorical label in its one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dP-sXgekAFOY"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "label_encoding = preprocessing.LabelBinarizer()\n",
    "train_labels = label_encoding.fit_transform(train_nodes)\n",
    "test_labels = label_encoding.transform(test_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJSpM4cnnfUN"
   },
   "source": [
    "It's now time for creating the mdoel. It will be composed by two GraphSAGE layers followed by a Dense layer with softmax activation for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BP5G49akANxy"
   },
   "outputs": [],
   "source": [
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "batchsize = 50\n",
    "n_samples = [10, 5, 7]\n",
    "generator = GraphSAGENodeGenerator(G, batchsize, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qkgF2VvWAlct"
   },
   "outputs": [],
   "source": [
    "from stellargraph.layer import GraphSAGE\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "graphsage_model = GraphSAGE(\n",
    "    layer_sizes=[32, 32, 16], generator=generator, bias=True, dropout=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S_g_yOhjAovl"
   },
   "outputs": [],
   "source": [
    "gnn_inp, gnn_out = graphsage_model.in_out_tensors()\n",
    "outputs = Dense(units=train_labels.shape[1], activation=\"softmax\")(gnn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HeMlOuDnA9B_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Model(inputs=gnn_inp, outputs=outputs)\n",
    "model.compile(optimizer=Adam(lr=0.003), loss=categorical_crossentropy, metrics=[\"acc\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqF4EWFbnwU9"
   },
   "source": [
    "We will use the flow function of the generator for feeding the model with the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "x-5xmzRqBDCX"
   },
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_nodes.index, train_labels, shuffle=True)\n",
    "test_gen = generator.flow(test_nodes.index, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "952-5V6Xn45o"
   },
   "source": [
    "Finally, let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sS3vnQ_HBZxK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 - 12s - loss: 1.8998 - acc: 0.1963 - val_loss: 1.8041 - val_acc: 0.3076\n",
      "Epoch 2/20\n",
      "6/6 - 10s - loss: 1.8258 - acc: 0.3148 - val_loss: 1.7662 - val_acc: 0.3093\n",
      "Epoch 3/20\n",
      "6/6 - 10s - loss: 1.7868 - acc: 0.3481 - val_loss: 1.7234 - val_acc: 0.3220\n",
      "Epoch 4/20\n",
      "6/6 - 10s - loss: 1.7427 - acc: 0.3407 - val_loss: 1.6617 - val_acc: 0.3991\n",
      "Epoch 5/20\n",
      "6/6 - 11s - loss: 1.6938 - acc: 0.3852 - val_loss: 1.5804 - val_acc: 0.5160\n",
      "Epoch 6/20\n",
      "6/6 - 10s - loss: 1.6212 - acc: 0.5037 - val_loss: 1.4891 - val_acc: 0.5853\n",
      "Epoch 7/20\n",
      "6/6 - 11s - loss: 1.5469 - acc: 0.6000 - val_loss: 1.3960 - val_acc: 0.6870\n",
      "Epoch 8/20\n",
      "6/6 - 11s - loss: 1.4961 - acc: 0.6222 - val_loss: 1.3267 - val_acc: 0.7559\n",
      "Epoch 9/20\n",
      "6/6 - 11s - loss: 1.3982 - acc: 0.7074 - val_loss: 1.2663 - val_acc: 0.7765\n",
      "Epoch 10/20\n",
      "6/6 - 11s - loss: 1.3558 - acc: 0.7296 - val_loss: 1.2178 - val_acc: 0.7646\n",
      "Epoch 11/20\n",
      "6/6 - 11s - loss: 1.2754 - acc: 0.7778 - val_loss: 1.1740 - val_acc: 0.7744\n",
      "Epoch 12/20\n",
      "6/6 - 13s - loss: 1.2252 - acc: 0.8000 - val_loss: 1.1290 - val_acc: 0.7838\n",
      "Epoch 13/20\n",
      "6/6 - 16s - loss: 1.1728 - acc: 0.8037 - val_loss: 1.0865 - val_acc: 0.7949\n",
      "Epoch 14/20\n",
      "6/6 - 14s - loss: 1.1230 - acc: 0.8037 - val_loss: 1.0472 - val_acc: 0.8015\n",
      "Epoch 15/20\n",
      "6/6 - 18s - loss: 1.0442 - acc: 0.8519 - val_loss: 1.0184 - val_acc: 0.7970\n",
      "Epoch 16/20\n",
      "6/6 - 21s - loss: 0.9898 - acc: 0.8741 - val_loss: 0.9956 - val_acc: 0.7904\n",
      "Epoch 17/20\n",
      "6/6 - 22s - loss: 0.9477 - acc: 0.8852 - val_loss: 0.9652 - val_acc: 0.7945\n",
      "Epoch 18/20\n",
      "6/6 - 23s - loss: 0.9245 - acc: 0.8444 - val_loss: 0.9461 - val_acc: 0.7982\n",
      "Epoch 19/20\n",
      "6/6 - 21s - loss: 0.8962 - acc: 0.8815 - val_loss: 0.9322 - val_acc: 0.7941\n",
      "Epoch 20/20\n",
      "6/6 - 22s - loss: 0.8339 - acc: 0.9407 - val_loss: 0.9110 - val_acc: 0.7945\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGJXHirZBcuL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFZJ2OJcoQgi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Supervised_GraphML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b632d4f284bbe77588fa149b6cb67b77182ce71d42208176a727bc04a09d6e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
